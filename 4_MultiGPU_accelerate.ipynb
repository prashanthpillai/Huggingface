{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10198a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d3050b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import datasets\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate import notebook_launcher\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b4efb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint) \n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698fa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True)\n",
    "\n",
    "def get_dataloaders(batch_size: int=64):\n",
    "    raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(['sentence1', 'sentence2', 'idx'])\n",
    "    tokenized_datasets = tokenized_datasets.rename_column('label', 'labels')\n",
    "    tokenized_datasets.set_format('torch')\n",
    "    \n",
    "    train_dataloader = DataLoader(tokenized_datasets['train'],\n",
    "                             shuffle=True,\n",
    "                             batch_size=batch_size,\n",
    "                             collate_fn=data_collator)\n",
    "    eval_dataloader = DataLoader(tokenized_datasets['validation'],                             \n",
    "                                 batch_size=batch_size,\n",
    "                                 collate_fn=data_collator)\n",
    "    \n",
    "    return train_dataloader, eval_dataloader, tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89813ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(seed: int = 42, batch_size: int = 64):\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()    \n",
    "        \n",
    "    # Build dataloaders\n",
    "    train_dataloader, eval_dataloader, tokenized_datasets = get_dataloaders(batch_size)\n",
    "    \n",
    "    checkpoint = \"bert-base-cased\"\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "    \n",
    "    # Intantiate the optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_scheduler(\n",
    "      \"linear\",\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=0,\n",
    "      num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    train_dataloader, eval_dataloader, model, optimizer, lr_scheduler = \\\n",
    "    accelerator.prepare(train_dataloader, eval_dataloader, model, optimizer, lr_scheduler)\n",
    "    \n",
    "    #progress_bar = tqdm(range(num_training_steps), disable=not accelerator.is_main_process)\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:        \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss        \n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            #progress_bar.update(1)\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for batch in eval_dataloader:\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)[:len(tokenized_datasets[\"validation\"])]\n",
    "        all_labels = torch.cat(all_labels)[:len(tokenized_datasets[\"validation\"])]\n",
    "\n",
    "        eval_metric = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfaaacf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching a training on 2 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38da118eb934070a27f83e394648b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ece218634f2dd18d.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cbdd35d6ef53475e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d1f2d1ca194a5ca0754d2b57e1d4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2489abcaef97d751.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ece218634f2dd18d.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cbdd35d6ef53475e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2489abcaef97d751.arrow\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'accuracy': 0.7156862745098039, 'f1': 0.81875}\n",
      "epoch 1: {'accuracy': 0.8333333333333334, 'f1': 0.8839590443686006}\n",
      "epoch 2: {'accuracy': 0.8357843137254902, 'f1': 0.8784029038112522}\n",
      "epoch 3: {'accuracy': 0.8529411764705882, 'f1': 0.896551724137931}\n",
      "epoch 4: {'accuracy': 0.8504901960784313, 'f1': 0.897133220910624}\n",
      "epoch 5: {'accuracy': 0.8553921568627451, 'f1': 0.899488926746167}\n",
      "epoch 6: {'accuracy': 0.8553921568627451, 'f1': 0.8963093145869947}\n",
      "epoch 7: {'accuracy': 0.8406862745098039, 'f1': 0.8929159802306424}\n",
      "epoch 8: {'accuracy': 0.8308823529411765, 'f1': 0.8851913477537438}\n",
      "epoch 9: {'accuracy': 0.8455882352941176, 'f1': 0.8884955752212389}\n"
     ]
    }
   ],
   "source": [
    "args = (42, 64)\n",
    "notebook_launcher(training_loop, args, num_processes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda77fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331bc3b6bc20454bade971cbc07a16cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-ece218634f2dd18d.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-cbdd35d6ef53475e.arrow\n",
      "Loading cached processed dataset at /home/jupyter/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2489abcaef97d751.arrow\n"
     ]
    }
   ],
   "source": [
    "# Build dataloaders\n",
    "train_dataloader, eval_dataloader, tokenized_datasets = get_dataloaders(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb1f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9fb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d1b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
